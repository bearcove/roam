//! Integration tests for file-backed mmap segments.
//!
//! These tests verify cross-process IPC using mmap.
//!
//! shm[verify shm.file.create]
//! shm[verify shm.file.attach]

use roam_core::MethodId;
use roam_shm::msg::ShmMsg;
use roam_shm::{AddPeerOptions, PollResult, SegmentConfig, ShmGuest, ShmHost, SpawnArgs, msg_type};

/// Test basic file-backed segment creation and attachment.
#[test]
fn test_file_backed_segment() {
    let dir = tempfile::tempdir().unwrap();
    let path = dir.path().join("test.shm");

    // Host creates segment
    let config = SegmentConfig::default();
    let mut host = ShmHost::create(&path, config).unwrap();

    // Verify file exists
    assert!(path.exists());
    assert_eq!(host.path(), Some(path.as_path()));

    // Guest attaches via path
    let mut guest = ShmGuest::attach_path(&path).unwrap();
    assert_eq!(guest.peer_id().get(), 1);

    // Send message host -> guest
    let msg = ShmMsg::new(msg_type::DATA, 1, MethodId(0), b"hello from host".to_vec());
    host.send(guest.peer_id(), &msg).unwrap();

    // Guest receives
    let received = guest.recv().unwrap();
    assert_eq!(received.payload_bytes(), b"hello from host");
}

/// Test bidirectional communication with file-backed segment.
#[test]
fn test_file_backed_bidirectional() {
    let dir = tempfile::tempdir().unwrap();
    let path = dir.path().join("bidir.shm");

    let config = SegmentConfig::default();
    let mut host = ShmHost::create(&path, config).unwrap();
    let mut guest = ShmGuest::attach_path(&path).unwrap();

    // Host -> Guest
    let msg = ShmMsg::new(msg_type::DATA, 1, MethodId(0), b"ping".to_vec());
    host.send(guest.peer_id(), &msg).unwrap();

    let received = guest.recv().unwrap();
    assert_eq!(received.payload_bytes(), b"ping");

    // Guest -> Host
    let msg = ShmMsg::new(msg_type::DATA, 2, MethodId(0), b"pong".to_vec());
    guest.send(&msg).unwrap();

    let PollResult { messages, .. } = host.poll();
    assert_eq!(messages.len(), 1);
    assert_eq!(messages[0].1.payload_bytes(), b"pong");
}

/// Test that file is cleaned up when host is dropped.
#[test]
fn test_file_cleanup_on_host_drop() {
    let dir = tempfile::tempdir().unwrap();
    let path = dir.path().join("cleanup.shm");

    {
        let _host = ShmHost::create(&path, SegmentConfig::default()).unwrap();
        assert!(path.exists());
    }

    // File should be deleted after host is dropped
    assert!(!path.exists());
}

/// Test large payload via slots with file-backed segment.
#[test]
fn test_file_backed_large_payload() {
    let dir = tempfile::tempdir().unwrap();
    let path = dir.path().join("large.shm");

    let config = SegmentConfig {
        max_payload_size: 64 * 1024 - 4,
        ..SegmentConfig::default()
    };
    let mut host = ShmHost::create(&path, config).unwrap();
    let mut guest = ShmGuest::attach_path(&path).unwrap();

    // Send large payload (requires slot)
    let large_payload = vec![0xAB; 1024];
    let msg = ShmMsg::new(msg_type::DATA, 1, MethodId(0), large_payload.clone());
    host.send(guest.peer_id(), &msg).unwrap();

    let received = guest.recv().unwrap();
    assert_eq!(received.payload_bytes(), large_payload.as_slice());
}

/// Test multiple guests with file-backed segment.
#[test]
fn test_file_backed_multiple_guests() {
    let dir = tempfile::tempdir().unwrap();
    let path = dir.path().join("multi.shm");

    let config = SegmentConfig {
        max_guests: 4,
        ..SegmentConfig::default()
    };
    let mut host = ShmHost::create(&path, config).unwrap();

    let mut guest1 = ShmGuest::attach_path(&path).unwrap();
    let mut guest2 = ShmGuest::attach_path(&path).unwrap();

    assert_eq!(guest1.peer_id().get(), 1);
    assert_eq!(guest2.peer_id().get(), 2);

    // Send to guest1
    let msg = ShmMsg::new(msg_type::DATA, 1, MethodId(0), b"for guest1".to_vec());
    host.send(guest1.peer_id(), &msg).unwrap();

    // Send to guest2
    let msg = ShmMsg::new(msg_type::DATA, 2, MethodId(0), b"for guest2".to_vec());
    host.send(guest2.peer_id(), &msg).unwrap();

    // Each guest receives their own message
    let msg1 = guest1.recv().unwrap();
    let msg2 = guest2.recv().unwrap();

    assert_eq!(msg1.payload_bytes(), b"for guest1");
    assert_eq!(msg2.payload_bytes(), b"for guest2");
}

/// Test cross-process IPC using fork.
///
/// This test forks a child process that attaches to the same segment
/// and exchanges messages with the parent (host).
///
/// shm[verify shm.architecture]
#[test]
#[cfg(unix)]
fn test_cross_process_ipc() {
    use std::time::Duration;

    let dir = tempfile::tempdir().unwrap();
    let path = dir.path().join("cross_process.shm");

    let config = SegmentConfig::default();
    let mut host = ShmHost::create(&path, config).unwrap();

    // Store path for child
    let path_str = path.to_str().unwrap().to_owned();

    match unsafe { libc::fork() } {
        -1 => panic!("fork failed"),
        0 => {
            // Child process: attach as guest and send message
            let path = std::path::Path::new(&path_str);
            let mut guest = ShmGuest::attach_path(path).unwrap();

            // Send message to host
            let msg = ShmMsg::new(msg_type::DATA, 1, MethodId(0), b"hello from child".to_vec());
            guest.send(&msg).unwrap();

            // Wait for response
            std::thread::sleep(Duration::from_millis(50));
            if let Some(response) = guest.recv() {
                assert_eq!(response.payload_bytes(), b"hello from parent");
            }

            // Exit child
            std::process::exit(0);
        }
        child_pid => {
            // Parent process: wait for message and respond
            std::thread::sleep(Duration::from_millis(100));

            let PollResult { messages, .. } = host.poll();
            assert_eq!(messages.len(), 1);
            assert_eq!(messages[0].1.payload_bytes(), b"hello from child");

            let peer_id = messages[0].0;

            // Send response
            let msg = ShmMsg::new(
                msg_type::DATA,
                2,
                MethodId(0),
                b"hello from parent".to_vec(),
            );
            host.send(peer_id, &msg).unwrap();

            // Wait for child to exit
            let mut status: i32 = 0;
            unsafe { libc::waitpid(child_pid, &mut status, 0) };

            assert!(libc::WIFEXITED(status), "child did not exit normally");
            assert_eq!(
                libc::WEXITSTATUS(status),
                0,
                "child exited with non-zero status"
            );
        }
    }
}

/// Test spawn ticket workflow with reserved slots.
///
/// This test simulates the spawn ticket pattern:
/// 1. Host reserves a slot via add_peer()
/// 2. Child process attaches using attach_with_ticket()
/// 3. Bidirectional communication works
///
/// shm[verify shm.spawn.ticket]
/// shm[verify shm.spawn.args]
/// shm[verify shm.spawn.guest-init]
/// shm[verify shm.spawn.reserved-state]
/// shm[verify shm.doorbell.socketpair]
#[test]
#[cfg(unix)]
fn test_spawn_ticket_workflow() {
    use std::sync::Arc;
    use std::sync::atomic::{AtomicBool, Ordering};
    use std::time::Duration;

    // Create a tokio runtime for doorbell AsyncFd support
    let rt = tokio::runtime::Builder::new_current_thread()
        .enable_all()
        .build()
        .unwrap();
    let _guard = rt.enter();

    let dir = tempfile::tempdir().unwrap();
    let path = dir.path().join("spawn_ticket.shm");

    let config = SegmentConfig::default();
    let mut host = ShmHost::create(&path, config).unwrap();

    // Track death callback invocation
    let death_called = Arc::new(AtomicBool::new(false));
    let death_called_clone = death_called.clone();

    // Host reserves a slot for the guest
    let ticket = host
        .add_peer(AddPeerOptions {
            peer_name: Some("test-guest".to_string()),
            on_death: Some(Arc::new(move |_peer_id| {
                death_called_clone.store(true, Ordering::SeqCst);
            })),
            ..Default::default()
        })
        .unwrap();

    assert_eq!(ticket.peer_id.get(), 1);

    // Get spawn args for child
    let args = ticket.to_args();
    assert_eq!(args.len(), 3);

    match unsafe { libc::fork() } {
        -1 => panic!("fork failed"),
        0 => {
            // Child process: parse args and attach with ticket
            let spawn_args = SpawnArgs::from_args(&args).unwrap();
            assert_eq!(spawn_args.peer_id.get(), 1);

            let mut guest = ShmGuest::attach_with_ticket(&spawn_args).unwrap();
            assert_eq!(guest.peer_id().get(), 1);

            // Send message to host
            let msg = ShmMsg::new(
                msg_type::DATA,
                1,
                MethodId(0),
                b"spawned guest says hi".to_vec(),
            );
            guest.send(&msg).unwrap();

            // Wait for response with retry
            for _ in 0..20 {
                std::thread::sleep(Duration::from_millis(50));
                if let Some(response) = guest.recv() {
                    assert_eq!(response.payload_bytes(), b"host acknowledges");
                    break;
                }
            }

            // Drop guest (triggers detach) and exit
            drop(guest);
            std::process::exit(0);
        }
        child_pid => {
            // Parent: drop ticket (closes our copy of guest's doorbell fd)
            drop(ticket);

            // Wait for message from child with retry
            let mut result = PollResult::default();
            for _ in 0..20 {
                std::thread::sleep(Duration::from_millis(50));
                result = host.poll();
                if !result.messages.is_empty() {
                    break;
                }
            }
            assert_eq!(result.messages.len(), 1, "expected 1 message from child");
            assert_eq!(
                result.messages[0].1.payload_bytes(),
                b"spawned guest says hi"
            );

            let peer_id = result.messages[0].0;
            assert_eq!(peer_id.get(), 1);

            // Send response
            let msg = ShmMsg::new(
                msg_type::DATA,
                2,
                MethodId(0),
                b"host acknowledges".to_vec(),
            );
            host.send(peer_id, &msg).unwrap();

            // Wait for child to exit
            let mut status: i32 = 0;
            unsafe { libc::waitpid(child_pid, &mut status, 0) };

            assert!(libc::WIFEXITED(status), "child did not exit normally");
            assert_eq!(
                libc::WEXITSTATUS(status),
                0,
                "child exited with non-zero status"
            );

            // After child exits, doorbell death detection should work
            std::thread::sleep(Duration::from_millis(50));
            let dead_peers = rt.block_on(host.check_doorbell_deaths());

            // The guest did a graceful detach, so it might not trigger death callback
            // But if the child crashed, the death callback would be invoked
            // For this test, we just verify the mechanism works
            let _ = dead_peers;
            let _ = death_called;
        }
    }
}

/// Test that releasing a reserved slot works if spawn fails.
#[test]
fn test_release_reserved_slot() {
    // Create a tokio runtime for doorbell AsyncFd support
    let rt = tokio::runtime::Builder::new_current_thread()
        .enable_all()
        .build()
        .unwrap();
    let _guard = rt.enter();

    let dir = tempfile::tempdir().unwrap();
    let path = dir.path().join("release.shm");

    let config = SegmentConfig::default();
    let mut host = ShmHost::create(&path, config).unwrap();

    // Reserve a slot
    let ticket = host
        .add_peer(AddPeerOptions {
            peer_name: Some("will-fail".to_string()),
            on_death: None,
            ..Default::default()
        })
        .unwrap();

    let peer_id = ticket.peer_id;
    assert_eq!(peer_id.get(), 1);

    // Simulate spawn failure - release the slot
    host.release_peer(peer_id);
    drop(ticket);

    // Slot should be available again
    let ticket2 = host
        .add_peer(AddPeerOptions {
            peer_name: Some("retry".to_string()),
            on_death: None,
            ..Default::default()
        })
        .unwrap();

    // Should get the same slot
    assert_eq!(ticket2.peer_id.get(), 1);
}

/// shm[verify shm.varslot.extents]
/// shm[verify shm.varslot.classes]
#[test]
fn test_grow_size_class() {
    let dir = tempfile::tempdir().unwrap();
    let path = dir.path().join("grow.shm");

    // Default config has variable-size slot pools configured
    let config = SegmentConfig {
        max_guests: 2, // Reduce to keep segment size reasonable
        ..SegmentConfig::default()
    };

    let mut host = ShmHost::create(&path, config).unwrap();

    // Get initial file size
    let initial_size = std::fs::metadata(&path).unwrap().len();

    // Grow the first size class
    let extent_idx = host.grow_size_class(0).unwrap();
    assert_eq!(extent_idx, 1); // First growth creates extent 1

    // File should be larger now
    let size_after_first_grow = std::fs::metadata(&path).unwrap().len();
    assert!(size_after_first_grow > initial_size);

    // Grow again
    let extent_idx = host.grow_size_class(0).unwrap();
    assert_eq!(extent_idx, 2); // Second growth creates extent 2

    let size_after_second_grow = std::fs::metadata(&path).unwrap().len();
    assert!(size_after_second_grow > size_after_first_grow);

    // Third growth should fail (max 3 extents)
    let result = host.grow_size_class(0);
    assert!(result.is_err());
}

/// Test that growing fails appropriately for heap-backed segments.
#[test]
fn test_grow_heap_backed_fails() {
    // Heap-backed segments can't grow - default has var_slot_classes
    let config = SegmentConfig {
        max_guests: 2,
        ..SegmentConfig::default()
    };

    let mut host = ShmHost::create_heap(config).unwrap();

    let result = host.grow_size_class(0);
    assert!(result.is_err());
}
